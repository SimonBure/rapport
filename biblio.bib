@article{amitModelGlobalSpontaneous1997,
  title = {Model of Global Spontaneous Activity and Local Structured Activity during Delay Periods in the Cerebral Cortex.},
  author = {Amit, D J and Brunel, N},
  year = {1997},
  month = apr,
  journal = {Cereb Cortex},
  volume = {7},
  number = {3},
  pages = {237--252},
  issn = {1047-3211},
  doi = {10.1093/cercor/7.3.237},
  urldate = {2025-08-13},
  abstract = {We investigate self-sustaining stable states (attractors) in networks of integrate-and-fire neurons. First, we study the stability of spontaneous activity in an unstructured network. It is shown that the stochastic background activity, of 1-5 spikes/s, is unstable if all neurons are excitatory. On the other hand, spontaneous activity becomes self-stabilizing in presence of local inhibition, given reasonable values of the parameters of the network. Second, in a network sustaining physiological spontaneous rates, we study the effect of learning in a local module, expressed in synaptic modifications in specific populations of synapses. We find that if the average synaptic potentiation (LTP) is too low, no stimulus specific activity manifests itself in the delay period. Instead, following the presentation and removal of any stimulus there is, in the local module, a delay activity in which all neurons selective (responding visually) to any of the stimuli presented for learning have rates which gradually increase with the amplitude of synaptic potentiation. When the average LTP increases beyond a critical value, specific local attractors (stable states) appear abruptly against the background of the global uniform spontaneous attractor. In this case the local module has two available types of collective delay activity: if the stimulus is unfamiliar, the activity is spontaneous; if it is similar to a learned stimulus, delay activity is selective. These new attractors reflect the synaptic structure developed during learning. In each of them a small population of neurons have elevated rates, which depend on the strength of LTP. The remaining neurons of the module have their activity at spontaneous rates. The predictions made in this paper could be checked by single unit recordings in delayed response experiments.}
}

@article{andreQuasiStationaryApproachMetastability2025,
  title = {A {{Quasi-Stationary Approach}} to {{Metastability}} in a {{System}} of {{Spiking Neurons}} with {{Synaptic Plasticity}}},
  author = {Andr{\'e}, Morgan and Pouzat, Christophe},
  year = {2025},
  month = jan,
  journal = {Mathematical Neuroscience and Applications},
  volume = {Volume 4},
  pages = {7668},
  issn = {2801-0159},
  doi = {10.46298/mna.7668},
  urldate = {2025-04-30},
  abstract = {After reviewing the behavioral studies of working memory and of the cellular substrate of the latter, we argue that metastable states constitute candidates for the type of transient information storage required by working memory. We then present a simple neural network model made of stochastic units whose synapses exhibit short-term facilitation. The Markov process dynamics of this model was specifically designed to be analytically tractable, simple to simulate numerically and to exhibit a quasistationary distribution (QSD). Since the state space is finite this QSD is also a Yaglom limit, which allows us to bridge the gap between quasi-stationarity and metastability by considering the relative orders of magnitude of the relaxation and absorption times. We present first analytical results: characterization of the absorbing region of the Markov process, irreducibility outside this absorbing region and consequently existence and uniqueness of a QSD. We then apply Perron-Frobenius spectral analysis to obtain any specific QSD, and design an approximate method for the first moments of this QSD when the exact method is intractable. Finally we use these methods to study the relaxation time toward the QSD and establish numerically the memorylessness of the time of extinction.},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/TWD53L3D/André and Pouzat - 2025 - A Quasi-Stationary Approach to Metastability in a System of Spiking Neurons with Synaptic Plasticity.pdf}
}

@article{baddeleyWorkingMemoryTheories2011,
  title = {Working {{Memory}}: {{Theories}}, {{Models}}, and {{Controversies}}},
  author = {Baddeley, Alan},
  year = {2011},
  month = sep,
  abstract = {I present an account of the origins and development of the multicomponent approach to working memory, making a distinction between the overall theoretical framework, which has remained relatively stable, and the attempts to build more specific models within this framework. I follow this with a brief discussion of alternative models and their relationship to the framework. I conclude with speculations on further developments and a comment on the value of attempting to apply models and theories beyond the laboratory studies on which they are typically based.},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/PUMJLXM3/Baddeley - Working Memory Theories, Models, and Controversies.pdf}
}

@article{barakWorkingModelsWorking2014,
  title = {Working Models of Working Memory},
  author = {Barak, Omri and Tsodyks, Misha},
  year = {2014},
  month = apr,
  journal = {Current Opinion in Neurobiology},
  volume = {25},
  pages = {20--24},
  issn = {09594388},
  doi = {10.1016/j.conb.2013.10.008},
  urldate = {2025-08-06},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/P5X8T544/Barak and Tsodyks - 2014 - Working models of working memory.pdf}
}

@article{bouchacourtFlexibleModelWorking2019,
  title = {A {{Flexible Model}} of {{Working Memory}}},
  author = {Bouchacourt, Flora and Buschman, Timothy J.},
  year = {2019},
  month = jul,
  journal = {Neuron},
  volume = {103},
  number = {1},
  pages = {147-160.e8},
  issn = {08966273},
  doi = {10.1016/j.neuron.2019.04.020},
  urldate = {2025-08-04},
  abstract = {Working memory is fundamental to cognition, allowing one to hold information ``in mind.'' A defining characteristic of working memory is its flexibility: we can hold anything in mind. However, typical models of working memory rely on finely tuned, content-specific attractors to persistently maintain neural activity and therefore do not allow for the flexibility observed in behavior. Here, we present a flexible model of working memory that maintains representations through random recurrent connections between two layers of neurons: a structured ``sensory'' layer and a randomly connected, unstructured layer. As the interactions are untuned with respect to the content being stored, the network maintains any arbitrary input. However, in our model, this flexibility comes at a cost: the random connections overlap, leading to interference between representations and limiting the memory capacity of the network. Additionally, our model captures several other key behavioral and neurophysiological characteristics of working memory.},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/8P9YL8R3/Bouchacourt and Buschman - 2019 - A Flexible Model of Working Memory.pdf}
}

@article{breakspearDynamicModelsLargescale2017,
  title = {Dynamic Models of Large-Scale Brain Activity},
  author = {Breakspear, Michael},
  year = {2017},
  month = mar,
  journal = {Nat Neurosci},
  volume = {20},
  number = {3},
  pages = {340--352},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4497},
  urldate = {2025-08-13},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/535ZYRR6/Breakspear - 2017 - Dynamic models of large-scale brain activity.pdf}
}

@book{bremaudDiscreteProbabilityModels2017,
  title = {Discrete {{Probability Models}} and {{Methods}}: {{Probability}} on {{Graphs}} and {{Trees}}, {{Markov Chains}} and {{Random Fields}}, {{Entropy}} and {{Coding}}},
  shorttitle = {Discrete {{Probability Models}} and {{Methods}}},
  author = {Br{\'e}maud, Pierre},
  year = {2017},
  series = {Probability {{Theory}} and {{Stochastic Modelling}}},
  volume = {78},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-43476-6},
  urldate = {2025-04-30},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-319-43475-9 978-3-319-43476-6},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/PK48Y3JI/Brémaud - 2017 - Discrete Probability Models and Methods Probability on Graphs and Trees, Markov Chains and Random F.pdf}
}

@article{brunelEffectsNeuromodulationCortical2001,
  title = {Effects of {{Neuromodulation}} in a {{Cortical Network Model}} of {{Object Working Memory Dominated}} by {{Recurrent Inhibition}}},
  author = {Brunel, Nicolas and Wang, Xiao-Jing},
  year = {2001},
  month = jul,
  journal = {J Comput Neurosci},
  volume = {11},
  number = {1},
  pages = {63--85},
  issn = {1573-6873},
  doi = {10.1023/A:1011204814320},
  urldate = {2025-08-13},
  abstract = {Experimental evidence suggests that the maintenance of an item in working memory is achieved through persistent activity in selective neural assemblies of the cortex. To understand the mechanisms underlying this phenomenon, it is essential to investigate how persistent activity is affected by external inputs or neuromodulation. We have addressed these questions using a recurrent network model of object working memory. Recurrence is dominated by inhibition, although persistent activity is generated through recurrent excitation in small subsets of excitatory neurons.},
  langid = {english},
  keywords = {AMPA,dopamine,GABA,inferotemporal cortex,network model,NMDA,persistent activity,prefrontal cortex,spontaneous activity,working memory}
}

@article{compteCellularNetworkMechanisms2003,
  title = {Cellular and {{Network Mechanisms}} of {{Slow Oscillatory Activity}} ({$<$}1 {{Hz}}) and {{Wave Propagations}} in a {{Cortical Network Model}}},
  author = {Compte, Albert and {Sanchez-Vives}, Maria V. and McCormick, David A. and Wang, Xiao-Jing},
  year = {2003},
  month = may,
  journal = {Journal of Neurophysiology},
  volume = {89},
  number = {5},
  pages = {2707--2725},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00845.2002},
  urldate = {2025-08-04},
  abstract = {Slow oscillatory activity ({$<$}1 Hz) is observed in vivo in the cortex during slow-wave sleep or under anesthesia and in vitro when the bath solution is chosen to more closely mimic cerebrospinal fluid. Here we present a biophysical network model for the slow oscillations observed in vitro that reproduces the single neuron behaviors and collective network firing patterns in control as well as under pharmacological manipulations. The membrane potential of a neuron oscillates slowly (at {$<$}1 Hz) between a down state and an up state; the up state is maintained by strong recurrent excitation balanced by inhibition, and the transition to the down state is due to a slow adaptation current (Na               +               -dependent K               +               current). Consistent with in vivo data, the input resistance of a model neuron, on average, is the largest at the end of the down state and the smallest during the initial phase of the up state. An activity wave is initiated by spontaneous spike discharges in a minority of neurons, and propagates across the network at a speed of 3--8 mm/s in control and 20--50 mm/s with inhibition block. Our work suggests that long-range excitatory patchy connections contribute significantly to this wave propagation. Finally, we show with this model that various known physiological effects of neuromodulation can switch the network to tonic firing, thus simulating a transition to the waking state.},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/CU9IBTU2/Compte et al. - 2003 - Cellular and Network Mechanisms of Slow Oscillatory Activity (1 Hz) and Wave Propagations in a Cort.pdf}
}

@article{darrochQuasiStationaryDistributionsAbsorbing1965,
  title = {On {{Quasi-Stationary}} Distributions in Absorbing Discrete-Time Finite {{Markov}} Chains},
  author = {Darroch, J. N. and Seneta, E.},
  year = {1965},
  month = jun,
  journal = {Journal of Applied Probability},
  volume = {2},
  number = {1},
  pages = {88--100},
  issn = {0021-9002, 1475-6072},
  doi = {10.2307/3211876},
  urldate = {2025-04-30},
  abstract = {The time to absorption from the set T of transient states of a Markov chain may be sufficiently long for the probability distribution over T to settle down in some sense to a "quasi-stationary" distribution. Various analogues of the stationary distribution of an irreducible chain are suggested and compared. The reverse process of an absorbing chain is found to be relevant.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/J4VMX5GG/Darroch and Seneta - 1965 - On Quasi-Stationary distributions in absorbing discrete-time finite Markov chains.pdf}
}

@article{dugginsConstructingFunctionalModels2022,
  title = {Constructing Functional Models from Biophysically-Detailed Neurons},
  author = {Duggins, Peter and Eliasmith, Chris},
  editor = {Migliore, Michele},
  year = {2022},
  month = sep,
  journal = {PLoS Comput Biol},
  volume = {18},
  number = {9},
  pages = {e1010461},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010461},
  urldate = {2025-08-04},
  abstract = {Improving biological plausibility and functional capacity are two important goals for brain models that connect low-level neural details to high-level behavioral phenomena. We develop a method called ``oracle-supervised Neural Engineering Framework'' (osNEF) to train biologically-detailed spiking neural networks that realize a variety of cognitively-relevant dynamical systems. Specifically, we train networks to perform computations that are commonly found in cognitive systems (communication, multiplication, harmonic oscillation, and gated working memory) using four distinct neuron models (leaky-integrate-and-fire neurons, Izhikevich neurons, 4-dimensional nonlinear point neurons, and 4-compartment, 6ion-channel layer-V pyramidal cell reconstructions) connected with various synaptic models (current-based synapses, conductance-based synapses, and voltage-gated synapses). We show that osNEF networks exhibit the target dynamics by accounting for nonlinearities present within the neuron models: performance is comparable across all four systems and all four neuron models, with variance proportional to task and neuron model complexity. We also apply osNEF to build a model of working memory that performs a delayed response task using a combination of pyramidal cells and inhibitory interneurons connected with NMDA and GABA synapses. The baseline performance and forgetting rate of the model are consistent with animal data from delayed match-to-sample tasks (DMTST): we observe a baseline performance of 95\% and exponential forgetting with time constant {$\tau$} = 8.5s, while a recent meta-analysis of DMTST performance across species observed baseline performances of 58 - 99\% and exponential forgetting with time constants of {$\tau$} = 2.4 - 71s. These results demonstrate that osNEF can train functional brain models using biologically-detailed components and open new avenues for investigating the relationship between biophysical mechanisms and functional capabilities.},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/G2BFWLYD/Duggins and Eliasmith - 2022 - Constructing functional models from biophysically-detailed neurons.pdf}
}

@article{durstewitzNeurocomputationalModelsWorking2000,
  title = {Neurocomputational Models of Working Memory},
  author = {Durstewitz, Daniel and Seamans, Jeremy K. and Sejnowski, Terrence J.},
  year = {2000},
  month = nov,
  journal = {Nat Neurosci},
  volume = {3},
  number = {11},
  pages = {1184--1191},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/81460},
  urldate = {2025-07-25},
  abstract = {During working memory tasks, the firing rates of single neurons recorded in behaving monkeys remain elevated without external cues. Modeling studies have explored different mechanisms that could underlie this selective persistent activity, including recurrent excitation within cell assemblies, synfire chains and single-cell bistability. The models show how sustained activity can be stable in the presence of noise and distractors, how different synaptic and voltage-gated conductances contribute to persistent activity, how neuromodulation could influence its robustness, how completely novel items could be maintained, and how continuous attractor states might be achieved. More work is needed to address the full repertoire of neural dynamics observed during working memory tasks.},
  copyright = {2000 Nature America Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/MB73YZC2/Durstewitz et al. - 2000 - Neurocomputational models of working memory.pdf}
}

@book{ericr.kandelPrinciplesNeuralSciences2013,
  title = {Principles of {{Neural Sciences}}},
  author = {Eric R. Kandel},
  year = {2013},
  edition = {5},
  publisher = {McGraw Hill},
  isbn = {978-0-07-139011-8}
}

@article{fusterNeuronActivityRelated1971,
  title = {Neuron Activity Related to Short-Term Memory},
  author = {Fuster, J. M. and Alexander, G. E.},
  year = {1971},
  month = aug,
  journal = {Science},
  volume = {173},
  number = {3997},
  pages = {652--654},
  issn = {0036-8075},
  doi = {10.1126/science.173.3997.652},
  abstract = {Nerve cells in the monkey's prefrontal cortex and nucleus medialis dorsalis of the thalamus show changes of firing frequency associated with the performance of a delayed response test. Most cells increase firing during the cue presentation period or at the beginning of the ensuing delay; spike discharge highler than that in intertrial periods is present in some cells throughout the delay. These changes are interpreted as suggestive evidence of a role of frontothalamic circuits in the attentive process involved in short-term memory},
  langid = {english},
  pmid = {4998337},
  keywords = {Action Potentials,Animals,Cerebral Cortex,Electrodes,Haplorhini,Memory Short-Term,Neurons,Thalamus}
}

@article{gordleevaModelingWorkingMemory2021,
  title = {Modeling {{Working Memory}} in a {{Spiking Neuron Network Accompanied}} by {{Astrocytes}}},
  author = {Gordleeva, Susanna Yu. and Tsybina, Yuliya A. and Krivonosov, Mikhail I. and Ivanchenko, Mikhail V. and Zaikin, Alexey A. and Kazantsev, Victor B. and Gorban, Alexander N.},
  year = {2021},
  month = mar,
  journal = {Front. Cell. Neurosci.},
  volume = {15},
  pages = {631485},
  issn = {1662-5102},
  doi = {10.3389/fncel.2021.631485},
  urldate = {2025-08-04},
  abstract = {We propose a novel biologically plausible computational model of working memory (WM) implemented by a spiking neuron network (SNN) interacting with a network of astrocytes. The SNN is modeled by synaptically coupled Izhikevich neurons with a non-specific architecture connection topology. Astrocytes generating calcium signals are connected by local gap junction diffusive couplings and interact with neurons via chemicals diffused in the extracellular space. Calcium elevations occur in response to the increased concentration of the neurotransmitter released by spiking neurons when a group of them fire coherently. In turn, gliotransmitters are released by activated astrocytes modulating the strength of the synaptic connections in the corresponding neuronal group. Input information is encoded as two-dimensional patterns of short applied current pulses stimulating neurons. The output is taken from frequencies of transient discharges of corresponding neurons. We show how a set of information patterns with quite significant overlapping areas can be uploaded into the neuron-astrocyte network and stored for several seconds. Information retrieval is organized by the application of a cue pattern representing one from the memory set distorted by noise. We found that successful retrieval with the level of the correlation between the recalled pattern and ideal pattern exceeding 90\% is possible for the multi-item WM task. Having analyzed the dynamical mechanism of WM formation, we discovered that astrocytes operating at a time scale of a dozen of seconds can successfully store traces of neuronal activations corresponding to information patterns. In the retrieval stage, the astrocytic network selectively modulates synaptic connections in the SNN leading to successful recall. Information and dynamical characteristics of the proposed WM model agrees with classical concepts and other WM models.},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/7EF9EA5C/Gordleeva et al. - 2021 - Modeling Working Memory in a Spiking Neuron Network Accompanied by Astrocytes.pdf}
}

@article{hitchMulticomponentModelWorking2025,
  title = {The Multicomponent Model of Working Memory Fifty Years On},
  author = {Hitch, Graham J. and Allen, Richard J. and Baddeley, Alan D.},
  year = {2025},
  month = feb,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {78},
  number = {2},
  pages = {222--239},
  publisher = {SAGE Publications},
  issn = {1747-0218, 1747-0226},
  doi = {10.1177/17470218241290909},
  urldate = {2025-07-25},
  abstract = {We provide a broad overview of our original investigation of working memory; how the multicomponent model followed from our use of a dissociative methodology; and our intention that it should be simple, robust, and applicable. We describe how subsequent development of the model has increased its scope, depth, and applications while at the same time retaining its core features. Comparisons with the growing number of alternative models suggest agreement on the basic phenomena to be explained and more similarities than differences. While differences between models attract interest, we caution that they do not necessarily reflect the most important issues for future research, which we suggest relate principally to the nature of executive control. The longevity of the multicomponent model reflects not only the importance of working memory in cognition but also the usefulness of a simple, robust framework for further theoretical development and applications.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/73LVB8XF/Hitch et al. - 2025 - The multicomponent model of working memory fifty years on.pdf}
}

@article{lauSynapticMechanismsPersistent2005,
  title = {Synaptic Mechanisms of Persistent Reverberatory Activity in Neuronal Networks},
  author = {Lau, Pak-Ming and Bi, Guo-Qiang},
  year = {2005},
  month = jul,
  journal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {102},
  number = {29},
  pages = {10333--10338},
  publisher = {Proceedings of the National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0500717102},
  urldate = {2025-07-25},
  abstract = {For brain functions such as working memory and motor planning, neuronal circuits are able to sustain persistent activity after transient inputs. Theoretical studies have suggested that persistent activity can exist in recurrently connected networks as active reverberation. However, the actual cellular processes underlying such reverberation are not well understood. In this study, we investigated the basic synaptic mechanisms responsible for reverberatory activity in small networks of rat hippocampal neurons            in vitro            . We found that brief stimulation of one neuron in a network could evoke, in an all-or-none fashion, reverberatory activity lasting for seconds. The reverberation was likely to arise from recurrent excitation because it was eliminated by partial inhibition of {$\alpha$}-amino-3-hydroxy-5-methyl-4-isoxazole propionic acid (AMPA)-type glutamate receptors (but not by blockade of NMDA receptors). In contrast, blocking inhibitory transmission with bicuculline enhanced the reverberation. Furthermore, paired-pulse stimuli with interpulse intervals of 200--400 ms were more effective than single pulses in triggering reverberation, apparently by eliciting higher levels of asynchronous transmitter release. Suppressing asynchronous release by EGTA-AM abolished reverberation, whereas elevating asynchronous release by strontium substantially enhanced reverberation. Finally, manipulating calcium uptake into or release from intracellular stores also modulated the level of reverberation. Thus, the oft-overlooked asynchronous phase of synaptic transmission plays a central role in the emergent phenomenon of network reverberation.},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/9HD4PAFN/Lau and Bi - 2005 - Synaptic mechanisms of persistent reverberatory activity in neuronal networks.pdf}
}

@article{lecompteSevenMinusTwo1999,
  title = {Seven, {{Plus}} or {{Minus Two}}, Is Too Much to {{Bear}}: {{Three}} (or {{Fewer}}) Is the {{Real Magic Number}}},
  shorttitle = {Seven, {{Plus}} or {{Minus Two}}, Is Too Much to {{Bear}}},
  author = {LeCompte, Denny C.},
  year = {1999},
  month = sep,
  journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  volume = {43},
  number = {3},
  pages = {289--292},
  publisher = {SAGE Publications Inc},
  issn = {1071-1813},
  doi = {10.1177/154193129904300334},
  urldate = {2025-08-13},
  abstract = {Miller's 7{\textpm}2 is an oft-cited but misleading heuristic in user interface design. Miller extracted this rule of thumb by examining data on memory span, among other things. However, memory span procedures measure short-term memory at its upper limits and thereby greatly overestimate humans' ability to easily hold in mind unrelated, linguistic material. Justifications for using a maximum of 3 items as a design heuristic are presented.},
  langid = {english}
}

@article{meleardQuasistationaryDistributionsPopulation2012,
  title = {Quasi-Stationary Distributions and Population Processes},
  author = {M{\'e}l{\'e}ard, Sylvie and Villemonais, Denis},
  year = {2012},
  month = jan,
  journal = {Probab. Surveys},
  volume = {9},
  number = {none},
  issn = {1549-5787},
  doi = {10.1214/11-PS191},
  urldate = {2025-08-09},
  abstract = {This survey concerns the study of quasi-stationary distributions with a specific focus on models derived from ecology and population dynamics. We are concerned with the long time behavior of different stochastic population size processes when 0 is an absorbing point almost surely attained by the process. The hitting time of this point, namely the extinction time, can be large compared to the physical time and the population size can fluctuate for large amount of time before extinction actually occurs. This phenomenon can be understood by the study of quasi-limiting distributions. In this paper, general results on quasi-stationarity are given and examples developed in detail. One shows in particular how this notion is related to the spectral properties of the semi-group of the process killed at 0. Then we study different stochastic population models including nonlinear terms modeling the regulation of the population. These models will take values in countable sets (as birth and death processes) or in continuous spaces (as logistic Feller diffusion processes or stochastic Lotka-Volterra processes). In all these situations we study in detail the quasi-stationarity properties. We also develop an algorithm based on Fleming-Viot particle systems and show a lot of numerical pictures.},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/SCYE869G/Méléard and Villemonais - 2012 - Quasi-stationary distributions and population processes.pdf}
}

@article{millerMagicalNumberSeven1994,
  title = {The Magical Number Seven, plus or Minus Two: {{Some}} Limits on Our Capacity for Processing Information},
  shorttitle = {The Magical Number Seven, plus or Minus Two},
  author = {Miller, George A.},
  year = {1994},
  journal = {Psychological Review},
  volume = {101},
  number = {2},
  pages = {343--352},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.101.2.343},
  abstract = {(This partially reprinted article originally appeared in Psychological Review, 1956, Vol 63, 81--97. The following abstract of the original article appeared in PA, Vol 31:2914.) A variety of researches are examined from the standpoint of information theory. It is shown that the unaided observer is severely limited in terms of the amount of information he can receive, process, and remember. However, it is shown that by the use of various techniques, e.g., use of several stimulus dimensions, recoding, and various mnemonic devices, this informational bottleneck can be broken. 20 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cognitive Processes,Human Channel Capacity,Information Theory},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/VP5ZGW7Q/1994-28291-001.html}
}

@article{millerWorkingMemory202018,
  title = {Working {{Memory}} 2.0},
  author = {Miller, Earl K. and Lundqvist, Mikael and Bastos, Andr{\'e} M.},
  year = {2018},
  month = oct,
  journal = {Neuron},
  volume = {100},
  number = {2},
  pages = {463--475},
  publisher = {Elsevier BV},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.09.023},
  urldate = {2025-07-25},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/LRVK24DF/Miller et al. - 2018 - Working Memory 2.0.pdf}
}

@article{mongilloSynapticTheoryWorking2008,
  title = {Synaptic {{Theory}} of {{Working Memory}}},
  author = {Mongillo, Gianluigi and Barak, Omri and Tsodyks, Misha},
  year = {2008},
  month = mar,
  journal = {Science},
  volume = {319},
  number = {5869},
  pages = {1543--1546},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1150769},
  urldate = {2025-08-06},
  abstract = {It is usually assumed that enhanced spiking activity in the form of persistent reverberation for several seconds is the neural correlate of working memory. Here, we propose that working memory is sustained by calcium-mediated synaptic facilitation in the recurrent connections of neocortical networks. In this account, the presynaptic residual calcium is used as a buffer that is loaded, refreshed, and read out by spiking activity. Because of the long time constants of calcium kinetics, the refresh rate can be low, resulting in a mechanism that is metabolically efficient and robust. The duration and stability of working memory can be regulated by modulating the spontaneous activity in the network.},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/VQ52TWI8/Mongillo et al. - 2008 - Synaptic Theory of Working Memory.pdf}
}

@article{murrayWorkingMemoryDecisionMaking2017,
  title = {Working {{Memory}} and {{Decision-Making}} in a {{Frontoparietal Circuit Model}}},
  author = {Murray, John D. and Jaramillo, Jorge and Wang, Xiao-Jing},
  year = {2017},
  month = dec,
  journal = {J. Neurosci.},
  volume = {37},
  number = {50},
  pages = {12167--12186},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0343-17.2017},
  urldate = {2025-08-04},
  abstract = {Working memory (WM) and decision-making (DM) are fundamental cognitive functions involving a distributed interacting network of brain areas, with the posterior parietal cortex (PPC) and prefrontal cortex (PFC) at the core. However, the shared and distinct roles of these areas and the nature of their coordination in cognitive function remain poorly understood. Biophysically based computational models of cortical circuits have provided insights into the mechanisms supporting these functions, yet they have primarily focused on the local microcircuit level, raising questions about the principles for distributed cognitive computation in multiregional networks. To examine these issues, we developed a distributed circuit model of two reciprocally interacting modules representing PPC and PFC circuits. The circuit architecture includes hierarchical differences in local recurrent structure and implements reciprocal long-range projections. This parsimonious model captures a range of behavioral and neuronal features of frontoparietal circuits across multiple WM and DM paradigms. In the context of WM, both areas exhibit persistent activity, but, in response to intervening distractors, PPC transiently encodes distractors while PFC filters distractors and supports WM robustness. With regard to DM, the PPC module generates graded representations of accumulated evidence supporting target selection, while the PFC module generates more categorical responses related to action or choice. These findings suggest computational principles for distributed, hierarchical processing in cortex during cognitive function and provide a framework for extension to multiregional models.                            SIGNIFICANCE STATEMENT               Working memory and decision-making are fundamental ``building blocks'' of cognition, and deficits in these functions are associated with neuropsychiatric disorders such as schizophrenia. These cognitive functions engage distributed networks with prefrontal cortex (PFC) and posterior parietal cortex (PPC) at the core. It is not clear, however, what the contributions of PPC and PFC are in light of the computations that subserve working memory and decision-making. We constructed a biophysical model of a reciprocally connected frontoparietal circuit that revealed shared and distinct functions for the PFC and PPC across working memory and decision-making tasks. Our parsimonious model connects circuit-level properties to cognitive functions and suggests novel design principles beyond those of local circuits for cognitive processing in multiregional brain networks.},
  copyright = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/9WI4FGDT/Murray et al. - 2017 - Working Memory and Decision-Making in a Frontoparietal Circuit Model.pdf}
}

@incollection{oberauerTheoryWorkingMemory2021,
  title = {Towards a Theory of Working Memory: {{From}} Metaphors to Mechanisms},
  shorttitle = {Towards a Theory of Working Memory},
  booktitle = {Working Memory: {{State}} of the Science},
  author = {Oberauer, Klaus},
  year = {2021},
  pages = {116--149},
  publisher = {Oxford University Press},
  address = {New York, NY, US},
  doi = {10.1093/oso/9780198842286.003.0005},
  abstract = {This chapter offers a conceptual framework that integrates a number of assumptions into a coherent narrative, so that it could serve as a blueprint for a theory. It incorporates some of the assumptions from the framework in computational models of working memory (WM). The function of WM is to enable us to build and manipulate temporary representations that deviate from those in long-term memory. The chapter presents a summary of the framework because it provides a good roadmap. It then discusses three computational models that incorporate some of the framework's assumptions to apply them to three areas of research: immediate serial recall of lists, immediate recall of simple visual arrays, and switching between memory sets and between task sets. The models are Serial Order in a Box--Complex Span that explains WM for lists in simple and complex span tasks; the interference model of visual WM that explains maintenance of arrays of simple visual objects; and the set-selection model that explains how the WM system selects memory sets and task sets, and how it selects elements within these sets. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  isbn = {978-0-19-884228-6},
  keywords = {Computational Modeling,Interference (Learning),Long Term Memory,Metaphor,Models,Short Term Memory,Theories},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/29AJJC7K/2021-20323-005.html}
}

@incollection{oreillyComputationalNeuroscienceModels2023,
  title = {Computational {{Neuroscience Models}} of {{Working Memory}}},
  booktitle = {The {{Cambridge Handbook}} of {{Computational Cognitive Sciences}}},
  author = {O'Reilly, Randall C. and Hazy, Thomas E. and Frank, Michael J.},
  year = {2023},
  month = may,
  edition = {2},
  pages = {611--663},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781108755610.023},
  urldate = {2025-07-25},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-1-108-75561-0},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/XEUU9NZY/2023 - Computational Neuroscience Models of Working Memory.pdf}
}

@article{pascanuNeurodynamicalModelWorking2011,
  title = {A Neurodynamical Model for Working Memory},
  author = {Pascanu, Razvan and Jaeger, Herbert},
  year = {2011},
  month = mar,
  journal = {Neural Networks},
  volume = {24},
  number = {2},
  pages = {199--207},
  publisher = {Elsevier BV},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2010.10.003},
  urldate = {2025-07-25},
  abstract = {Neurodynamical models of working memory (WM) should provide mechanisms for storing, maintaining, retrieving, and deleting information. Many models address only a subset of these aspects. Here we present a rather simple WM model where all of these performance modes are trained into a recurrent neural network (RNN) of the Echo State Network (ESN) type. The model is demonstrated on a bracket level parsing task with a stream of rich and noisy graphical script input. In terms of nonlinear dynamics, memory states correspond, intuitively, to attractors in an input-driven system. As a supplementary contribution, the article proposes a rigorous formal framework to describe such attractors, generalizing from the standard definition of attractors in autonomous (input-free) dynamical systems.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/3A3S334L/Pascanu and Jaeger - 2011 - A neurodynamical model for working memory.pdf}
}

@misc{ramsauerHopfieldNetworksAll2021,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  year = {2021},
  month = apr,
  number = {arXiv:2008.02217},
  eprint = {2008.02217},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2008.02217},
  urldate = {2025-08-13},
  abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/NJ4AT2BN/Ramsauer et al. - 2021 - Hopfield Networks is All You Need.pdf;/home/syy/snap/zotero-snap/common/Zotero/storage/GWA5KJ6Q/2008.html}
}

@article{stroudComputationalFoundationsDynamic2024,
  title = {The Computational Foundations of Dynamic Coding in Working Memory},
  author = {Stroud, Jake P. and Duncan, John and Lengyel, M{\'a}t{\'e}},
  year = {2024},
  month = jul,
  journal = {Trends in Cognitive Sciences},
  volume = {28},
  number = {7},
  pages = {614--627},
  publisher = {Elsevier BV},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2024.02.011},
  urldate = {2025-07-25},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/VJCGQ3LM/Stroud et al. - 2024 - The computational foundations of dynamic coding in working memory.pdf}
}

@article{stroudOptimalInformationLoading2023,
  title = {Optimal Information Loading into Working Memory Explains Dynamic Coding in the Prefrontal Cortex},
  author = {Stroud, Jake P. and Watanabe, Kei and Suzuki, Takafumi and Stokes, Mark G. and Lengyel, M{\'a}t{\'e}},
  year = {2023},
  month = nov,
  journal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {120},
  number = {48},
  publisher = {Proceedings of the National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2307991120},
  urldate = {2025-07-25},
  abstract = {Working memory involves the short-term maintenance of information and is critical in many tasks. The neural circuit dynamics underlying working memory remain poorly understood, with different aspects of prefrontal cortical (PFC) responses explained by different putative mechanisms. By mathematical analysis, numerical simulations, and using recordings from monkey PFC, we investigate a critical but hitherto ignored aspect of working memory dynamics: information loading. We find that, contrary to common assumptions, optimal loading of information into working memory involves inputs that are largely orthogonal, rather than similar, to the late delay activities observed during memory maintenance, naturally leading to the widely observed phenomenon of dynamic coding in PFC. Using a theoretically principled metric, we show that PFC exhibits the hallmarks of optimal information loading. We also find that optimal information loading emerges as a general dynamical strategy in task-optimized recurrent neural networks. Our theory unifies previous, seemingly conflicting theories of memory maintenance based on attractor or purely sequential dynamics and reveals a normative principle underlying dynamic coding.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/4V6GG3MF/Stroud et al. - 2023 - Optimal information loading into working memory explains dynamic coding in the prefrontal cortex.pdf}
}

@article{wangSynapticReverberationUnderlying2001,
  title = {Synaptic Reverberation Underlying Mnemonic Persistent Activity},
  author = {Wang, Xiao-Jing},
  year = {2001},
  month = aug,
  journal = {Trends in Neurosciences},
  volume = {24},
  number = {8},
  pages = {455--463},
  publisher = {Elsevier},
  issn = {0166-2236, 1878-108X},
  doi = {10.1016/S0166-2236(00)01868-3},
  urldate = {2025-07-25},
  langid = {english},
  pmid = {11476885},
  keywords = {biophysical model,dopamine modulation,dynamical stability,Neuroscience,NMDA receptors,persistent activity,working memory},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/TACIZ7GJ/Wang - 2001 - Synaptic reverberation underlying mnemonic persistent activity.pdf}
}

@article{yangSAMUnifiedSelfAdaptive2022,
  title = {{{SAM}}: {{A Unified Self-Adaptive Multicompartmental Spiking Neuron Model}} for {{Learning With Working Memory}}},
  shorttitle = {{{SAM}}},
  author = {Yang, Shuangming and Gao, Tian and Wang, Jiang and Deng, Bin and Azghadi, Mostafa Rahimi and Lei, Tao and {Linares-Barranco}, Bernabe},
  year = {2022},
  month = apr,
  journal = {Front. Neurosci.},
  volume = {16},
  pages = {850945},
  issn = {1662-453X},
  doi = {10.3389/fnins.2022.850945},
  urldate = {2025-08-04},
  abstract = {Working memory is a fundamental feature of biological brains for perception, cognition, and learning. In addition, learning with working memory, which has been show in conventional artificial intelligence systems through recurrent neural networks, is instrumental to advanced cognitive intelligence. However, it is hard to endow a simple neuron model with working memory, and to understand the biological mechanisms that have resulted in such a powerful ability at the neuronal level. This article presents a novel self-adaptive multicompartment spiking neuron model, referred to as SAM, for spike-based learning with working memory. SAM integrates four major biological principles including sparse coding, dendritic non-linearity, intrinsic self-adaptive dynamics, and spike-driven learning. We first describe SAM's design and explore the impacts of critical parameters on its biological dynamics. We then use SAM to build spiking networks to accomplish several different tasks including supervised learning of the MNIST dataset using sequential spatiotemporal encoding, noisy spike pattern classification, sparse coding during pattern classification, spatiotemporal feature detection, meta-learning with working memory applied to a navigation task and the MNIST classification task, and working memory for spatiotemporal learning. Our experimental results highlight the energy efficiency and robustness of SAM in these wide range of challenging tasks. The effects of SAM model variations on its working memory are also explored, hoping to offer insight into the biological mechanisms underlying working memory in the brain. The SAM model is the first attempt to integrate the capabilities of spike-driven learning and working memory in a unified single neuron with multiple timescale dynamics. The competitive performance of SAM could potentially contribute to the development of efficient adaptive neuromorphic computing systems for various applications from robotics to edge computing.},
  langid = {english},
  file = {/home/syy/snap/zotero-snap/common/Zotero/storage/LDMKG7U5/Yang et al. - 2022 - SAM A Unified Self-Adaptive Multicompartmental Spiking Neuron Model for Learning With Working Memor.pdf}
}
